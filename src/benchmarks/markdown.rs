//! Markdown report generation for benchmark results
//!
//! This module generates human-readable markdown reports from benchmark results.

use crate::benchmarks::BenchmarkResult;
use chrono::Utc;
use std::fmt::Write as FmtWrite;
use std::fs::{self, File};
use std::io::Write;
use std::path::Path;

/// Default path for the summary markdown file
pub const SUMMARY_PATH: &str = "benchmarks/output/summary.md";

/// Generate a markdown summary from benchmark results
pub fn generate_summary(results: &[BenchmarkResult]) -> String {
    let mut output = String::new();

    // Header
    writeln!(output, "# LLM Incident Manager Benchmark Results").unwrap();
    writeln!(output).unwrap();
    writeln!(
        output,
        "*Generated: {}*",
        Utc::now().format("%Y-%m-%d %H:%M:%S UTC")
    )
    .unwrap();
    writeln!(output).unwrap();

    // Summary statistics
    writeln!(output, "## Summary").unwrap();
    writeln!(output).unwrap();

    let total = results.len();
    let successful = results.iter().filter(|r| r.is_success()).count();
    let failed = total - successful;

    writeln!(output, "| Metric | Value |").unwrap();
    writeln!(output, "|--------|-------|").unwrap();
    writeln!(output, "| Total Benchmarks | {} |", total).unwrap();
    writeln!(output, "| Successful | {} |", successful).unwrap();
    writeln!(output, "| Failed | {} |", failed).unwrap();
    writeln!(output).unwrap();

    // Detailed results table
    writeln!(output, "## Detailed Results").unwrap();
    writeln!(output).unwrap();

    writeln!(
        output,
        "| Target | Status | Duration (ms) | Throughput | Timestamp |"
    )
    .unwrap();
    writeln!(output, "|--------|--------|---------------|------------|-----------|").unwrap();

    for result in results {
        let status = if result.is_success() { "Pass" } else { "Fail" };
        let duration = result
            .duration_ms()
            .map(|d| format!("{:.2}", d))
            .unwrap_or_else(|| "-".to_string());
        let throughput = result
            .throughput()
            .map(|t| format!("{:.2}", t))
            .unwrap_or_else(|| "-".to_string());
        let timestamp = result.timestamp.format("%Y-%m-%d %H:%M:%S");

        writeln!(
            output,
            "| {} | {} | {} | {} | {} |",
            result.target_id, status, duration, throughput, timestamp
        )
        .unwrap();
    }
    writeln!(output).unwrap();

    // Detailed metrics for each target
    writeln!(output, "## Individual Benchmark Details").unwrap();
    writeln!(output).unwrap();

    for result in results {
        writeln!(output, "### {}", result.target_id).unwrap();
        writeln!(output).unwrap();
        writeln!(
            output,
            "**Executed:** {}",
            result.timestamp.format("%Y-%m-%d %H:%M:%S UTC")
        )
        .unwrap();
        writeln!(output).unwrap();

        writeln!(output, "**Metrics:**").unwrap();
        writeln!(output, "```json").unwrap();
        writeln!(
            output,
            "{}",
            serde_json::to_string_pretty(&result.metrics).unwrap_or_else(|_| "{}".to_string())
        )
        .unwrap();
        writeln!(output, "```").unwrap();
        writeln!(output).unwrap();
    }

    // Footer
    writeln!(output, "---").unwrap();
    writeln!(output).unwrap();
    writeln!(
        output,
        "*This report was generated by the LLM Incident Manager canonical benchmark system.*"
    )
    .unwrap();

    output
}

/// Write the summary to the default location
pub fn write_summary(results: &[BenchmarkResult]) -> std::io::Result<()> {
    write_summary_to_path(results, SUMMARY_PATH)
}

/// Write the summary to a specific path
pub fn write_summary_to_path(results: &[BenchmarkResult], path: impl AsRef<Path>) -> std::io::Result<()> {
    let path = path.as_ref();

    // Ensure parent directory exists
    if let Some(parent) = path.parent() {
        fs::create_dir_all(parent)?;
    }

    let content = generate_summary(results);
    let mut file = File::create(path)?;
    file.write_all(content.as_bytes())?;

    Ok(())
}

/// Generate a comparison report between two sets of results
pub fn generate_comparison(
    baseline: &[BenchmarkResult],
    current: &[BenchmarkResult],
) -> String {
    let mut output = String::new();

    writeln!(output, "# Benchmark Comparison Report").unwrap();
    writeln!(output).unwrap();
    writeln!(
        output,
        "*Generated: {}*",
        Utc::now().format("%Y-%m-%d %H:%M:%S UTC")
    )
    .unwrap();
    writeln!(output).unwrap();

    writeln!(
        output,
        "| Target | Baseline (ms) | Current (ms) | Change (%) |"
    )
    .unwrap();
    writeln!(output, "|--------|---------------|--------------|------------|").unwrap();

    // Create a map of baseline results by target_id
    let baseline_map: std::collections::HashMap<_, _> = baseline
        .iter()
        .map(|r| (r.target_id.as_str(), r))
        .collect();

    for result in current {
        let baseline_duration = baseline_map
            .get(result.target_id.as_str())
            .and_then(|r| r.duration_ms());
        let current_duration = result.duration_ms();

        let (baseline_str, current_str, change_str) = match (baseline_duration, current_duration) {
            (Some(b), Some(c)) => {
                let change = ((c - b) / b) * 100.0;
                let change_indicator = if change > 0.0 {
                    format!("+{:.1}% (slower)", change)
                } else if change < 0.0 {
                    format!("{:.1}% (faster)", change)
                } else {
                    "0.0%".to_string()
                };
                (format!("{:.2}", b), format!("{:.2}", c), change_indicator)
            }
            (None, Some(c)) => ("-".to_string(), format!("{:.2}", c), "new".to_string()),
            (Some(b), None) => (format!("{:.2}", b), "-".to_string(), "missing".to_string()),
            (None, None) => ("-".to_string(), "-".to_string(), "-".to_string()),
        };

        writeln!(
            output,
            "| {} | {} | {} | {} |",
            result.target_id, baseline_str, current_str, change_str
        )
        .unwrap();
    }

    output
}

#[cfg(test)]
mod tests {
    use super::*;

    #[test]
    fn test_generate_summary() {
        let results = vec![
            BenchmarkResult::new(
                "test-1",
                serde_json::json!({
                    "duration_ms": 100.0,
                    "throughput": 1000.0
                }),
            ),
            BenchmarkResult::new(
                "test-2",
                serde_json::json!({
                    "duration_ms": 50.0,
                    "error": "test error"
                }),
            ),
        ];

        let summary = generate_summary(&results);
        assert!(summary.contains("LLM Incident Manager Benchmark Results"));
        assert!(summary.contains("test-1"));
        assert!(summary.contains("test-2"));
        assert!(summary.contains("Total Benchmarks | 2"));
    }

    #[test]
    fn test_generate_comparison() {
        let baseline = vec![BenchmarkResult::new(
            "test-target",
            serde_json::json!({ "duration_ms": 100.0 }),
        )];

        let current = vec![BenchmarkResult::new(
            "test-target",
            serde_json::json!({ "duration_ms": 90.0 }),
        )];

        let comparison = generate_comparison(&baseline, &current);
        assert!(comparison.contains("Benchmark Comparison Report"));
        assert!(comparison.contains("test-target"));
    }
}
